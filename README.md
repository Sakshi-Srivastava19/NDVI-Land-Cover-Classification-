# NDVI-Land-Cover-Classification-
# Summer Analytics 2025 - NDVI Land Cover Classification ğŸŒ¿

This repository contains my solution for the **Summer Analytics 2025 Hackathon**, hosted by Consulting & Analytics Club (IITG) and GeeksforGeeks.

---

## Problem Overview

The task was to classify land cover types using NDVI time-series data derived from satellite imagery. The challenge involved dealing with:

- Noisy NDVI signals (due to cloud cover and crowdsourced labeling)
- Missing data
- Seasonal vegetation patterns

The goal was to build a **Logistic Regression-based multiclass model** (competition rule) for predicting 6 land cover classes:
Water, Impervious, Farm, Forest, Grass, Orchard.

---

## Solution Approach

- âœ… **Data Imputation:** Median imputation for missing NDVI values.
- âœ… **Noise Reduction:** Applied rolling window smoothing.
- âœ… **Feature Engineering:** Statistical, seasonal, and trend-based features.
- âœ… **Dimensionality Reduction:** PCA (n=5 components).
- âœ… **Polynomial Expansion:** Degree 2 polynomial features.
- âœ… **Model Training:** Logistic Regression with 5-fold Stratified CV.
## Approach Summary

### 1ï¸âƒ£ Data Preprocessing

- Median imputation of missing NDVI values.
- Rolling window smoothing to reduce short-term noise fluctuations.
- Post-smoothing re-imputation to handle residual missing data.

### 2ï¸âƒ£ Feature Engineering

- Statistical features: mean, std, min, max, range, skew, kurtosis, quartiles.
- Time-series trend: NDVI slope via linear regression.
- Seasonal features: summer vs winter NDVI averages and difference.

### 3ï¸âƒ£ Dimensionality Reduction

- Applied PCA (n=5) to compress 27 NDVI columns into orthogonal principal components, reducing multicollinearity and improving feature compactness.

### 4ï¸âƒ£ Non-Linear Feature Expansion

- Polynomial Features (degree=2) to enhance non-linearity within the logistic regression framework.

### 5ï¸âƒ£ Model Training

- Scaled features with StandardScaler.
- Logistic Regression with lbfgs solver, multinomial objective.
- 5-fold Stratified Cross-Validation for robust evaluation.

---
---

## Model Performance

- Average CV Accuracy: **~89.4%**

| Fold | Accuracy |
| ---- | -------- |
| 1    | 89.31%   |
| 2    | 90.00%   |
| 3    | 89.69%   |
| 4    | 88.75%   |
| 5    | 89.50%   |

---

## Tools Used

- Python
- Pandas, Numpy, SciPy
- Scikit-learn

---

## Submission File

- `submission.csv` â€” final predictions for leaderboard evaluation.

---

ğŸ¯ **Special thanks to the organizers for this amazing learning experience!**

---

## Contact

Feel free to connect with me on LinkedIn or GitHub ğŸ˜Š







ğŸŒŸ My First Kaggle Hackathon Experience! ğŸš€ ğŸŒŸ

Iâ€™m excited to share that I recently participated in my first-ever Kaggle Hackathon â€” the Summer Analytics First Hackathon organized by Consulting & Analytics Club & GeeksforGeeks!

ğŸ’»ğŸ”¬ Competition Theme:
ğŸŒ± NDVI-based Land Cover Classification using satellite imagery data

ğŸ¯ Key Highlights:

âœ… First Kaggle competition experience âœ…

âœ… Explored real-world geospatial and time-series data âœ…

âœ… Built my own Machine Learning pipeline from scratch âœ…

âœ… Scored 0.61333 and secured a leaderboard rank of 227 out of hundreds of participants ğŸ‰

âœ… Learned the power of feature engineering, data cleaning, and model experimentation

ğŸ”§ Skills Sharpened:

Python ğŸ

Pandas, NumPy, Scikit-learn ğŸ“Š

Machine Learning Models (Logistic Regression, Random Forest, XGBoost)

Data Analysis & Problem Solving ğŸ”

This journey has taught me a lot about tackling real-world datasets, model optimization, and continuous learning. The Kaggle community has been incredibly inspiring, and I can't wait to take part in more competitions to sharpen my skills further! ğŸ”¥

A big shoutout to the organizers and fellow participants for making this an amazing learning experience! ğŸ™Œ

#Kaggle #MachineLearning #DataScience #Hackathon #FirstHackathon #KaggleJourney #NDVI #AI #ContinuousLearning #ProudMoment
